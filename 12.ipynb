{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72324e7f",
   "metadata": {},
   "source": [
    "### Create a Linear Regression model with PyTorch Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256b607",
   "metadata": {},
   "source": [
    "1. Data gathering\n",
    "2. Data preprocessing\n",
    "3. Feature engineering\n",
    "4. Model training\n",
    "5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aef72447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cu126\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9411a7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/thinkpad/.cache/kagglehub/datasets/mirichoi0218/insurance/versions/1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"mirichoi0218/insurance\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0f823dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insurance.csv']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1338 entries, 0 to 1337\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1338 non-null   int64  \n",
      " 1   sex       1338 non-null   object \n",
      " 2   bmi       1338 non-null   float64\n",
      " 3   children  1338 non-null   int64  \n",
      " 4   smoker    1338 non-null   object \n",
      " 5   region    1338 non-null   object \n",
      " 6   charges   1338 non-null   float64\n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 73.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(os.listdir(path))\n",
    "df=pd.read_csv(os.path.join(path, \"insurance.csv\"))\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ec7d54e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     sex     bmi  children smoker     region      charges\n",
      "0   19  female  27.900         0    yes  southwest  16884.92400\n",
      "1   18    male  33.770         1     no  southeast   1725.55230\n",
      "2   28    male  33.000         3     no  southeast   4449.46200\n",
      "3   33    male  22.705         0     no  northwest  21984.47061\n",
      "4   32    male  28.880         0     no  northwest   3866.85520\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eb4c800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               age          bmi     children       charges\n",
      "count  1338.000000  1338.000000  1338.000000   1338.000000\n",
      "mean     39.207025    30.663397     1.094918  13270.422265\n",
      "std      14.049960     6.098187     1.205493  12110.011237\n",
      "min      18.000000    15.960000     0.000000   1121.873900\n",
      "25%      27.000000    26.296250     0.000000   4740.287150\n",
      "50%      39.000000    30.400000     1.000000   9382.033000\n",
      "75%      51.000000    34.693750     2.000000  16639.912515\n",
      "max      64.000000    53.130000     5.000000  63770.428010\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "49e9ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4d02f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset before encoding\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# encode categorical features\n",
    "label_encoder = {}\n",
    "categorical_features = ['sex', 'smoker', 'region']\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_df[feature] = le.fit_transform(train_df[feature])\n",
    "    test_df[feature] = le.transform(test_df[feature])\n",
    "    label_encoder[feature] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2c52ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target\n",
    "X_train = train_df.drop(columns=['charges'])\n",
    "y_train = train_df['charges']\n",
    "X_test = test_df.drop(columns=['charges'])\n",
    "y_test = test_df['charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6a919820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age  sex    bmi  children  smoker  region\n",
      "560    46    0  19.95         2       0       1\n",
      "1285   47    0  24.32         0       0       0\n",
      "1142   52    0  24.86         0       0       2\n",
      "969    39    0  34.32         5       0       2\n",
      "486    54    0  21.47         3       0       1\n",
      "560      9193.83850\n",
      "1285     8534.67180\n",
      "1142    27117.99378\n",
      "969      8596.82780\n",
      "486     12475.35130\n",
      "Name: charges, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "611b2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5b50a2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.47222651 -1.0246016  -1.75652513  0.73433626 -0.50874702 -0.45611589]\n",
      " [ 0.54331294 -1.0246016  -1.03308239 -0.91119211 -0.50874702 -1.35325561]\n",
      " [ 0.8987451  -1.0246016  -0.94368672 -0.91119211 -0.50874702  0.44102382]\n",
      " ...\n",
      " [ 1.3252637   0.97598911 -0.89153925 -0.91119211 -0.50874702 -1.35325561]\n",
      " [-0.16755139 -1.0246016   2.82086429  0.73433626  1.96561348  1.33816354]\n",
      " [ 1.1120044   0.97598911 -0.10932713 -0.91119211 -0.50874702  1.33816354]]\n",
      "560      9193.83850\n",
      "1285     8534.67180\n",
      "1142    27117.99378\n",
      "969      8596.82780\n",
      "486     12475.35130\n",
      "           ...     \n",
      "1095     4561.18850\n",
      "1130     8582.30230\n",
      "1294    11931.12525\n",
      "860     46113.51100\n",
      "1126    10214.63600\n",
      "Name: charges, Length: 1070, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "416fdb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor=torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor=torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor=torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor=torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d017b4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9193.8389],\n",
      "        [ 8534.6719],\n",
      "        [27117.9941],\n",
      "        ...,\n",
      "        [11931.1250],\n",
      "        [46113.5117],\n",
      "        [10214.6357]])\n",
      "torch.Size([1070, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_train_tensor)\n",
    "print(y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fb5fda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "class SimpleNNRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "10564a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1070, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train_tensor.shape)\n",
    "X_train_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "353cd9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNNRegression(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNNRegression(input_size=X_train_tensor.shape[1])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1030a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0aae75f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/50000, Loss: 320772448.0000\n",
      "Epoch 200/50000, Loss: 307184928.0000\n",
      "Epoch 300/50000, Loss: 267343488.0000\n",
      "Epoch 400/50000, Loss: 199529808.0000\n",
      "Epoch 500/50000, Loss: 125031224.0000\n",
      "Epoch 600/50000, Loss: 73518040.0000\n",
      "Epoch 700/50000, Loss: 50417896.0000\n",
      "Epoch 800/50000, Loss: 41650476.0000\n",
      "Epoch 900/50000, Loss: 37707528.0000\n",
      "Epoch 1000/50000, Loss: 35675728.0000\n",
      "Epoch 1100/50000, Loss: 34553748.0000\n",
      "Epoch 1200/50000, Loss: 33862000.0000\n",
      "Epoch 1300/50000, Loss: 33357552.0000\n",
      "Epoch 1400/50000, Loss: 32933908.0000\n",
      "Epoch 1500/50000, Loss: 32532636.0000\n",
      "Epoch 1600/50000, Loss: 32132098.0000\n",
      "Epoch 1700/50000, Loss: 31714344.0000\n",
      "Epoch 1800/50000, Loss: 31264830.0000\n",
      "Epoch 1900/50000, Loss: 30772780.0000\n",
      "Epoch 2000/50000, Loss: 30223924.0000\n",
      "Epoch 2100/50000, Loss: 29600640.0000\n",
      "Epoch 2200/50000, Loss: 28905346.0000\n",
      "Epoch 2300/50000, Loss: 28159830.0000\n",
      "Epoch 2400/50000, Loss: 27382398.0000\n",
      "Epoch 2500/50000, Loss: 26584640.0000\n",
      "Epoch 2600/50000, Loss: 25786896.0000\n",
      "Epoch 2700/50000, Loss: 25021748.0000\n",
      "Epoch 2800/50000, Loss: 24321936.0000\n",
      "Epoch 2900/50000, Loss: 23700654.0000\n",
      "Epoch 3000/50000, Loss: 23165974.0000\n",
      "Epoch 3100/50000, Loss: 22717822.0000\n",
      "Epoch 3200/50000, Loss: 22324084.0000\n",
      "Epoch 3300/50000, Loss: 21990714.0000\n",
      "Epoch 3400/50000, Loss: 21712354.0000\n",
      "Epoch 3500/50000, Loss: 21470090.0000\n",
      "Epoch 3600/50000, Loss: 21261434.0000\n",
      "Epoch 3700/50000, Loss: 21081140.0000\n",
      "Epoch 3800/50000, Loss: 20925696.0000\n",
      "Epoch 3900/50000, Loss: 20801220.0000\n",
      "Epoch 4000/50000, Loss: 20691324.0000\n",
      "Epoch 4100/50000, Loss: 20591392.0000\n",
      "Epoch 4200/50000, Loss: 20498754.0000\n",
      "Epoch 4300/50000, Loss: 20411262.0000\n",
      "Epoch 4400/50000, Loss: 20322752.0000\n",
      "Epoch 4500/50000, Loss: 20238696.0000\n",
      "Epoch 4600/50000, Loss: 20158446.0000\n",
      "Epoch 4700/50000, Loss: 20079088.0000\n",
      "Epoch 4800/50000, Loss: 19993664.0000\n",
      "Epoch 4900/50000, Loss: 19912858.0000\n",
      "Epoch 5000/50000, Loss: 19827532.0000\n",
      "Epoch 5100/50000, Loss: 19736732.0000\n",
      "Epoch 5200/50000, Loss: 19633642.0000\n",
      "Epoch 5300/50000, Loss: 19530162.0000\n",
      "Epoch 5400/50000, Loss: 19430974.0000\n",
      "Epoch 5500/50000, Loss: 19335134.0000\n",
      "Epoch 5600/50000, Loss: 19244368.0000\n",
      "Epoch 5700/50000, Loss: 19155836.0000\n",
      "Epoch 5800/50000, Loss: 19074460.0000\n",
      "Epoch 5900/50000, Loss: 18992574.0000\n",
      "Epoch 6000/50000, Loss: 18911340.0000\n",
      "Epoch 6100/50000, Loss: 18824924.0000\n",
      "Epoch 6200/50000, Loss: 18741286.0000\n",
      "Epoch 6300/50000, Loss: 18653598.0000\n",
      "Epoch 6400/50000, Loss: 18570876.0000\n",
      "Epoch 6500/50000, Loss: 18485416.0000\n",
      "Epoch 6600/50000, Loss: 18398434.0000\n",
      "Epoch 6700/50000, Loss: 18317206.0000\n",
      "Epoch 6800/50000, Loss: 18236054.0000\n",
      "Epoch 6900/50000, Loss: 18153836.0000\n",
      "Epoch 7000/50000, Loss: 18069212.0000\n",
      "Epoch 7100/50000, Loss: 17975280.0000\n",
      "Epoch 7200/50000, Loss: 17889814.0000\n",
      "Epoch 7300/50000, Loss: 17803196.0000\n",
      "Epoch 7400/50000, Loss: 17695994.0000\n",
      "Epoch 7500/50000, Loss: 17600474.0000\n",
      "Epoch 7600/50000, Loss: 17488984.0000\n",
      "Epoch 7700/50000, Loss: 17385494.0000\n",
      "Epoch 7800/50000, Loss: 17281558.0000\n",
      "Epoch 7900/50000, Loss: 17181412.0000\n",
      "Epoch 8000/50000, Loss: 17075924.0000\n",
      "Epoch 8100/50000, Loss: 16972116.0000\n",
      "Epoch 8200/50000, Loss: 16869318.0000\n",
      "Epoch 8300/50000, Loss: 16772728.0000\n",
      "Epoch 8400/50000, Loss: 16663462.0000\n",
      "Epoch 8500/50000, Loss: 16560117.0000\n",
      "Epoch 8600/50000, Loss: 16460021.0000\n",
      "Epoch 8700/50000, Loss: 16358741.0000\n",
      "Epoch 8800/50000, Loss: 16248319.0000\n",
      "Epoch 8900/50000, Loss: 16146983.0000\n",
      "Epoch 9000/50000, Loss: 16043931.0000\n",
      "Epoch 9100/50000, Loss: 15931249.0000\n",
      "Epoch 9200/50000, Loss: 15811208.0000\n",
      "Epoch 9300/50000, Loss: 15684217.0000\n",
      "Epoch 9400/50000, Loss: 15555944.0000\n",
      "Epoch 9500/50000, Loss: 15427759.0000\n",
      "Epoch 9600/50000, Loss: 15299548.0000\n",
      "Epoch 9700/50000, Loss: 15162746.0000\n",
      "Epoch 9800/50000, Loss: 15038420.0000\n",
      "Epoch 9900/50000, Loss: 14912993.0000\n",
      "Epoch 10000/50000, Loss: 14772731.0000\n",
      "Epoch 10100/50000, Loss: 14645877.0000\n",
      "Epoch 10200/50000, Loss: 14535861.0000\n",
      "Epoch 10300/50000, Loss: 14428879.0000\n",
      "Epoch 10400/50000, Loss: 14326322.0000\n",
      "Epoch 10500/50000, Loss: 14215827.0000\n",
      "Epoch 10600/50000, Loss: 14114564.0000\n",
      "Epoch 10700/50000, Loss: 14012788.0000\n",
      "Epoch 10800/50000, Loss: 13909973.0000\n",
      "Epoch 10900/50000, Loss: 13811701.0000\n",
      "Epoch 11000/50000, Loss: 13716663.0000\n",
      "Epoch 11100/50000, Loss: 13623191.0000\n",
      "Epoch 11200/50000, Loss: 13525063.0000\n",
      "Epoch 11300/50000, Loss: 13427365.0000\n",
      "Epoch 11400/50000, Loss: 13327482.0000\n",
      "Epoch 11500/50000, Loss: 13230196.0000\n",
      "Epoch 11600/50000, Loss: 13133832.0000\n",
      "Epoch 11700/50000, Loss: 13041729.0000\n",
      "Epoch 11800/50000, Loss: 12945775.0000\n",
      "Epoch 11900/50000, Loss: 12848324.0000\n",
      "Epoch 12000/50000, Loss: 12753295.0000\n",
      "Epoch 12100/50000, Loss: 12665116.0000\n",
      "Epoch 12200/50000, Loss: 12569433.0000\n",
      "Epoch 12300/50000, Loss: 12475440.0000\n",
      "Epoch 12400/50000, Loss: 12381488.0000\n",
      "Epoch 12500/50000, Loss: 12286385.0000\n",
      "Epoch 12600/50000, Loss: 12188429.0000\n",
      "Epoch 12700/50000, Loss: 12095498.0000\n",
      "Epoch 12800/50000, Loss: 12009125.0000\n",
      "Epoch 12900/50000, Loss: 11923293.0000\n",
      "Epoch 13000/50000, Loss: 11842443.0000\n",
      "Epoch 13100/50000, Loss: 11762914.0000\n",
      "Epoch 13200/50000, Loss: 11672359.0000\n",
      "Epoch 13300/50000, Loss: 11564716.0000\n",
      "Epoch 13400/50000, Loss: 11476037.0000\n",
      "Epoch 13500/50000, Loss: 11387211.0000\n",
      "Epoch 13600/50000, Loss: 11300530.0000\n",
      "Epoch 13700/50000, Loss: 11216833.0000\n",
      "Epoch 13800/50000, Loss: 11121848.0000\n",
      "Epoch 13900/50000, Loss: 11008211.0000\n",
      "Epoch 14000/50000, Loss: 10901768.0000\n",
      "Epoch 14100/50000, Loss: 10813047.0000\n",
      "Epoch 14200/50000, Loss: 10727259.0000\n",
      "Epoch 14300/50000, Loss: 10647665.0000\n",
      "Epoch 14400/50000, Loss: 10569269.0000\n",
      "Epoch 14500/50000, Loss: 10492381.0000\n",
      "Epoch 14600/50000, Loss: 10414600.0000\n",
      "Epoch 14700/50000, Loss: 10335028.0000\n",
      "Epoch 14800/50000, Loss: 10253042.0000\n",
      "Epoch 14900/50000, Loss: 10168889.0000\n",
      "Epoch 15000/50000, Loss: 10092090.0000\n",
      "Epoch 15100/50000, Loss: 10020111.0000\n",
      "Epoch 15200/50000, Loss: 9932630.0000\n",
      "Epoch 15300/50000, Loss: 9848333.0000\n",
      "Epoch 15400/50000, Loss: 9765864.0000\n",
      "Epoch 15500/50000, Loss: 9675686.0000\n",
      "Epoch 15600/50000, Loss: 9599566.0000\n",
      "Epoch 15700/50000, Loss: 9526616.0000\n",
      "Epoch 15800/50000, Loss: 9463143.0000\n",
      "Epoch 15900/50000, Loss: 9401041.0000\n",
      "Epoch 16000/50000, Loss: 9342076.0000\n",
      "Epoch 16100/50000, Loss: 9281959.0000\n",
      "Epoch 16200/50000, Loss: 9213698.0000\n",
      "Epoch 16300/50000, Loss: 9153205.0000\n",
      "Epoch 16400/50000, Loss: 9090674.0000\n",
      "Epoch 16500/50000, Loss: 9035289.0000\n",
      "Epoch 16600/50000, Loss: 8983501.0000\n",
      "Epoch 16700/50000, Loss: 8932814.0000\n",
      "Epoch 16800/50000, Loss: 8877437.0000\n",
      "Epoch 16900/50000, Loss: 8823476.0000\n",
      "Epoch 17000/50000, Loss: 8773265.0000\n",
      "Epoch 17100/50000, Loss: 8726160.0000\n",
      "Epoch 17200/50000, Loss: 8679371.0000\n",
      "Epoch 17300/50000, Loss: 8634592.0000\n",
      "Epoch 17400/50000, Loss: 8591976.0000\n",
      "Epoch 17500/50000, Loss: 8551016.0000\n",
      "Epoch 17600/50000, Loss: 8510568.0000\n",
      "Epoch 17700/50000, Loss: 8471726.0000\n",
      "Epoch 17800/50000, Loss: 8433589.0000\n",
      "Epoch 17900/50000, Loss: 8392881.0000\n",
      "Epoch 18000/50000, Loss: 8348087.5000\n",
      "Epoch 18100/50000, Loss: 8297639.5000\n",
      "Epoch 18200/50000, Loss: 8254707.0000\n",
      "Epoch 18300/50000, Loss: 8213646.5000\n",
      "Epoch 18400/50000, Loss: 8173053.0000\n",
      "Epoch 18500/50000, Loss: 8134124.0000\n",
      "Epoch 18600/50000, Loss: 8094438.5000\n",
      "Epoch 18700/50000, Loss: 8054437.5000\n",
      "Epoch 18800/50000, Loss: 8015641.0000\n",
      "Epoch 18900/50000, Loss: 7979562.5000\n",
      "Epoch 19000/50000, Loss: 7940970.0000\n",
      "Epoch 19100/50000, Loss: 7898391.0000\n",
      "Epoch 19200/50000, Loss: 7857418.0000\n",
      "Epoch 19300/50000, Loss: 7810360.0000\n",
      "Epoch 19400/50000, Loss: 7762236.0000\n",
      "Epoch 19500/50000, Loss: 7717083.0000\n",
      "Epoch 19600/50000, Loss: 7675599.5000\n",
      "Epoch 19700/50000, Loss: 7620049.0000\n",
      "Epoch 19800/50000, Loss: 7568130.0000\n",
      "Epoch 19900/50000, Loss: 7524635.5000\n",
      "Epoch 20000/50000, Loss: 7481995.0000\n",
      "Epoch 20100/50000, Loss: 7435461.0000\n",
      "Epoch 20200/50000, Loss: 7393686.5000\n",
      "Epoch 20300/50000, Loss: 7348373.0000\n",
      "Epoch 20400/50000, Loss: 7308257.5000\n",
      "Epoch 20500/50000, Loss: 7273340.0000\n",
      "Epoch 20600/50000, Loss: 7239721.0000\n",
      "Epoch 20700/50000, Loss: 7203733.0000\n",
      "Epoch 20800/50000, Loss: 7169927.5000\n",
      "Epoch 20900/50000, Loss: 7136620.5000\n",
      "Epoch 21000/50000, Loss: 7102663.5000\n",
      "Epoch 21100/50000, Loss: 7069746.0000\n",
      "Epoch 21200/50000, Loss: 7036197.0000\n",
      "Epoch 21300/50000, Loss: 7003596.5000\n",
      "Epoch 21400/50000, Loss: 6972748.5000\n",
      "Epoch 21500/50000, Loss: 6942045.5000\n",
      "Epoch 21600/50000, Loss: 6911240.5000\n",
      "Epoch 21700/50000, Loss: 6878817.0000\n",
      "Epoch 21800/50000, Loss: 6848011.5000\n",
      "Epoch 21900/50000, Loss: 6817917.5000\n",
      "Epoch 22000/50000, Loss: 6788757.5000\n",
      "Epoch 22100/50000, Loss: 6759292.5000\n",
      "Epoch 22200/50000, Loss: 6730183.5000\n",
      "Epoch 22300/50000, Loss: 6701206.5000\n",
      "Epoch 22400/50000, Loss: 6670726.5000\n",
      "Epoch 22500/50000, Loss: 6642695.5000\n",
      "Epoch 22600/50000, Loss: 6616541.5000\n",
      "Epoch 22700/50000, Loss: 6589904.0000\n",
      "Epoch 22800/50000, Loss: 6562776.0000\n",
      "Epoch 22900/50000, Loss: 6537019.0000\n",
      "Epoch 23000/50000, Loss: 6511660.0000\n",
      "Epoch 23100/50000, Loss: 6485208.5000\n",
      "Epoch 23200/50000, Loss: 6459152.0000\n",
      "Epoch 23300/50000, Loss: 6433442.5000\n",
      "Epoch 23400/50000, Loss: 6408636.5000\n",
      "Epoch 23500/50000, Loss: 6383418.5000\n",
      "Epoch 23600/50000, Loss: 6359650.5000\n",
      "Epoch 23700/50000, Loss: 6335368.5000\n",
      "Epoch 23800/50000, Loss: 6311436.5000\n",
      "Epoch 23900/50000, Loss: 6287933.5000\n",
      "Epoch 24000/50000, Loss: 6259621.0000\n",
      "Epoch 24100/50000, Loss: 6232842.0000\n",
      "Epoch 24200/50000, Loss: 6205699.5000\n",
      "Epoch 24300/50000, Loss: 6179080.0000\n",
      "Epoch 24400/50000, Loss: 6154229.5000\n",
      "Epoch 24500/50000, Loss: 6130079.5000\n",
      "Epoch 24600/50000, Loss: 6101468.0000\n",
      "Epoch 24700/50000, Loss: 6075553.0000\n",
      "Epoch 24800/50000, Loss: 6051912.5000\n",
      "Epoch 24900/50000, Loss: 6030547.5000\n",
      "Epoch 25000/50000, Loss: 6009283.0000\n",
      "Epoch 25100/50000, Loss: 5988790.0000\n",
      "Epoch 25200/50000, Loss: 5968326.5000\n",
      "Epoch 25300/50000, Loss: 5948395.0000\n",
      "Epoch 25400/50000, Loss: 5929585.0000\n",
      "Epoch 25500/50000, Loss: 5909758.5000\n",
      "Epoch 25600/50000, Loss: 5891400.5000\n",
      "Epoch 25700/50000, Loss: 5872871.0000\n",
      "Epoch 25800/50000, Loss: 5852724.5000\n",
      "Epoch 25900/50000, Loss: 5832267.0000\n",
      "Epoch 26000/50000, Loss: 5810058.0000\n",
      "Epoch 26100/50000, Loss: 5788933.5000\n",
      "Epoch 26200/50000, Loss: 5769581.5000\n",
      "Epoch 26300/50000, Loss: 5745517.5000\n",
      "Epoch 26400/50000, Loss: 5724863.0000\n",
      "Epoch 26500/50000, Loss: 5702222.5000\n",
      "Epoch 26600/50000, Loss: 5679768.0000\n",
      "Epoch 26700/50000, Loss: 5660406.5000\n",
      "Epoch 26800/50000, Loss: 5641411.5000\n",
      "Epoch 26900/50000, Loss: 5622431.5000\n",
      "Epoch 27000/50000, Loss: 5603835.0000\n",
      "Epoch 27100/50000, Loss: 5585353.0000\n",
      "Epoch 27200/50000, Loss: 5560341.0000\n",
      "Epoch 27300/50000, Loss: 5530946.0000\n",
      "Epoch 27400/50000, Loss: 5510697.0000\n",
      "Epoch 27500/50000, Loss: 5490378.0000\n",
      "Epoch 27600/50000, Loss: 5466561.0000\n",
      "Epoch 27700/50000, Loss: 5440300.5000\n",
      "Epoch 27800/50000, Loss: 5414328.5000\n",
      "Epoch 27900/50000, Loss: 5391053.0000\n",
      "Epoch 28000/50000, Loss: 5372850.5000\n",
      "Epoch 28100/50000, Loss: 5352297.0000\n",
      "Epoch 28200/50000, Loss: 5334221.0000\n",
      "Epoch 28300/50000, Loss: 5317860.0000\n",
      "Epoch 28400/50000, Loss: 5299096.5000\n",
      "Epoch 28500/50000, Loss: 5280801.0000\n",
      "Epoch 28600/50000, Loss: 5263219.5000\n",
      "Epoch 28700/50000, Loss: 5243383.5000\n",
      "Epoch 28800/50000, Loss: 5223440.0000\n",
      "Epoch 28900/50000, Loss: 5205748.0000\n",
      "Epoch 29000/50000, Loss: 5189284.5000\n",
      "Epoch 29100/50000, Loss: 5173398.5000\n",
      "Epoch 29200/50000, Loss: 5158102.5000\n",
      "Epoch 29300/50000, Loss: 5143213.5000\n",
      "Epoch 29400/50000, Loss: 5127029.5000\n",
      "Epoch 29500/50000, Loss: 5111459.0000\n",
      "Epoch 29600/50000, Loss: 5095622.5000\n",
      "Epoch 29700/50000, Loss: 5080080.5000\n",
      "Epoch 29800/50000, Loss: 5065554.0000\n",
      "Epoch 29900/50000, Loss: 5051186.0000\n",
      "Epoch 30000/50000, Loss: 5036760.0000\n",
      "Epoch 30100/50000, Loss: 5020680.5000\n",
      "Epoch 30200/50000, Loss: 5005987.0000\n",
      "Epoch 30300/50000, Loss: 4991435.0000\n",
      "Epoch 30400/50000, Loss: 4975602.5000\n",
      "Epoch 30500/50000, Loss: 4961110.5000\n",
      "Epoch 30600/50000, Loss: 4947010.5000\n",
      "Epoch 30700/50000, Loss: 4933172.5000\n",
      "Epoch 30800/50000, Loss: 4918601.5000\n",
      "Epoch 30900/50000, Loss: 4905403.0000\n",
      "Epoch 31000/50000, Loss: 4890138.0000\n",
      "Epoch 31100/50000, Loss: 4875412.0000\n",
      "Epoch 31200/50000, Loss: 4862206.5000\n",
      "Epoch 31300/50000, Loss: 4849096.5000\n",
      "Epoch 31400/50000, Loss: 4836610.0000\n",
      "Epoch 31500/50000, Loss: 4823913.5000\n",
      "Epoch 31600/50000, Loss: 4811073.5000\n",
      "Epoch 31700/50000, Loss: 4799061.5000\n",
      "Epoch 31800/50000, Loss: 4786608.5000\n",
      "Epoch 31900/50000, Loss: 4774693.0000\n",
      "Epoch 32000/50000, Loss: 4762913.5000\n",
      "Epoch 32100/50000, Loss: 4751037.0000\n",
      "Epoch 32200/50000, Loss: 4740014.5000\n",
      "Epoch 32300/50000, Loss: 4728247.0000\n",
      "Epoch 32400/50000, Loss: 4717517.0000\n",
      "Epoch 32500/50000, Loss: 4706716.5000\n",
      "Epoch 32600/50000, Loss: 4695327.0000\n",
      "Epoch 32700/50000, Loss: 4682705.5000\n",
      "Epoch 32800/50000, Loss: 4671204.0000\n",
      "Epoch 32900/50000, Loss: 4660113.0000\n",
      "Epoch 33000/50000, Loss: 4649418.0000\n",
      "Epoch 33100/50000, Loss: 4638853.0000\n",
      "Epoch 33200/50000, Loss: 4628168.5000\n",
      "Epoch 33300/50000, Loss: 4616179.0000\n",
      "Epoch 33400/50000, Loss: 4604087.5000\n",
      "Epoch 33500/50000, Loss: 4592806.5000\n",
      "Epoch 33600/50000, Loss: 4582029.5000\n",
      "Epoch 33700/50000, Loss: 4569932.5000\n",
      "Epoch 33800/50000, Loss: 4559088.0000\n",
      "Epoch 33900/50000, Loss: 4546942.5000\n",
      "Epoch 34000/50000, Loss: 4535488.5000\n",
      "Epoch 34100/50000, Loss: 4524178.0000\n",
      "Epoch 34200/50000, Loss: 4513998.0000\n",
      "Epoch 34300/50000, Loss: 4504357.0000\n",
      "Epoch 34400/50000, Loss: 4494437.5000\n",
      "Epoch 34500/50000, Loss: 4484905.5000\n",
      "Epoch 34600/50000, Loss: 4475289.5000\n",
      "Epoch 34700/50000, Loss: 4465850.5000\n",
      "Epoch 34800/50000, Loss: 4456617.0000\n",
      "Epoch 34900/50000, Loss: 4446266.0000\n",
      "Epoch 35000/50000, Loss: 4436627.0000\n",
      "Epoch 35100/50000, Loss: 4427496.0000\n",
      "Epoch 35200/50000, Loss: 4418229.5000\n",
      "Epoch 35300/50000, Loss: 4406651.0000\n",
      "Epoch 35400/50000, Loss: 4394212.0000\n",
      "Epoch 35500/50000, Loss: 4381838.5000\n",
      "Epoch 35600/50000, Loss: 4370096.5000\n",
      "Epoch 35700/50000, Loss: 4358553.5000\n",
      "Epoch 35800/50000, Loss: 4347987.5000\n",
      "Epoch 35900/50000, Loss: 4337501.0000\n",
      "Epoch 36000/50000, Loss: 4327899.5000\n",
      "Epoch 36100/50000, Loss: 4318003.0000\n",
      "Epoch 36200/50000, Loss: 4308707.5000\n",
      "Epoch 36300/50000, Loss: 4299047.5000\n",
      "Epoch 36400/50000, Loss: 4289551.0000\n",
      "Epoch 36500/50000, Loss: 4279852.5000\n",
      "Epoch 36600/50000, Loss: 4271031.0000\n",
      "Epoch 36700/50000, Loss: 4261383.5000\n",
      "Epoch 36800/50000, Loss: 4252948.0000\n",
      "Epoch 36900/50000, Loss: 4243839.5000\n",
      "Epoch 37000/50000, Loss: 4234894.5000\n",
      "Epoch 37100/50000, Loss: 4225752.5000\n",
      "Epoch 37200/50000, Loss: 4217234.0000\n",
      "Epoch 37300/50000, Loss: 4208935.5000\n",
      "Epoch 37400/50000, Loss: 4200977.5000\n",
      "Epoch 37500/50000, Loss: 4193056.5000\n",
      "Epoch 37600/50000, Loss: 4184765.5000\n",
      "Epoch 37700/50000, Loss: 4176949.0000\n",
      "Epoch 37800/50000, Loss: 4169401.2500\n",
      "Epoch 37900/50000, Loss: 4162317.0000\n",
      "Epoch 38000/50000, Loss: 4154698.7500\n",
      "Epoch 38100/50000, Loss: 4147424.5000\n",
      "Epoch 38200/50000, Loss: 4140634.0000\n",
      "Epoch 38300/50000, Loss: 4132932.5000\n",
      "Epoch 38400/50000, Loss: 4126131.5000\n",
      "Epoch 38500/50000, Loss: 4119074.0000\n",
      "Epoch 38600/50000, Loss: 4112869.7500\n",
      "Epoch 38700/50000, Loss: 4106175.5000\n",
      "Epoch 38800/50000, Loss: 4099945.7500\n",
      "Epoch 38900/50000, Loss: 4092863.5000\n",
      "Epoch 39000/50000, Loss: 4086425.0000\n",
      "Epoch 39100/50000, Loss: 4080109.2500\n",
      "Epoch 39200/50000, Loss: 4073541.7500\n",
      "Epoch 39300/50000, Loss: 4067034.7500\n",
      "Epoch 39400/50000, Loss: 4060969.7500\n",
      "Epoch 39500/50000, Loss: 4054661.0000\n",
      "Epoch 39600/50000, Loss: 4047751.0000\n",
      "Epoch 39700/50000, Loss: 4041550.5000\n",
      "Epoch 39800/50000, Loss: 4035323.2500\n",
      "Epoch 39900/50000, Loss: 4027555.7500\n",
      "Epoch 40000/50000, Loss: 4020624.5000\n",
      "Epoch 40100/50000, Loss: 4013911.7500\n",
      "Epoch 40200/50000, Loss: 4006875.5000\n",
      "Epoch 40300/50000, Loss: 4000524.0000\n",
      "Epoch 40400/50000, Loss: 3994191.5000\n",
      "Epoch 40500/50000, Loss: 3988061.5000\n",
      "Epoch 40600/50000, Loss: 3981748.2500\n",
      "Epoch 40700/50000, Loss: 3975482.7500\n",
      "Epoch 40800/50000, Loss: 3969494.2500\n",
      "Epoch 40900/50000, Loss: 3962133.2500\n",
      "Epoch 41000/50000, Loss: 3956026.0000\n",
      "Epoch 41100/50000, Loss: 3950149.5000\n",
      "Epoch 41200/50000, Loss: 3944002.7500\n",
      "Epoch 41300/50000, Loss: 3938139.5000\n",
      "Epoch 41400/50000, Loss: 3932452.7500\n",
      "Epoch 41500/50000, Loss: 3927086.7500\n",
      "Epoch 41600/50000, Loss: 3921495.5000\n",
      "Epoch 41700/50000, Loss: 3915802.7500\n",
      "Epoch 41800/50000, Loss: 3910695.5000\n",
      "Epoch 41900/50000, Loss: 3904534.7500\n",
      "Epoch 42000/50000, Loss: 3898996.5000\n",
      "Epoch 42100/50000, Loss: 3893325.0000\n",
      "Epoch 42200/50000, Loss: 3888734.5000\n",
      "Epoch 42300/50000, Loss: 3882426.2500\n",
      "Epoch 42400/50000, Loss: 3876889.7500\n",
      "Epoch 42500/50000, Loss: 3871837.0000\n",
      "Epoch 42600/50000, Loss: 3866204.5000\n",
      "Epoch 42700/50000, Loss: 3861154.5000\n",
      "Epoch 42800/50000, Loss: 3855620.7500\n",
      "Epoch 42900/50000, Loss: 3850277.7500\n",
      "Epoch 43000/50000, Loss: 3845375.2500\n",
      "Epoch 43100/50000, Loss: 3839660.7500\n",
      "Epoch 43200/50000, Loss: 3833687.5000\n",
      "Epoch 43300/50000, Loss: 3828515.0000\n",
      "Epoch 43400/50000, Loss: 3823134.5000\n",
      "Epoch 43500/50000, Loss: 3817552.7500\n",
      "Epoch 43600/50000, Loss: 3812460.0000\n",
      "Epoch 43700/50000, Loss: 3807160.2500\n",
      "Epoch 43800/50000, Loss: 3802135.0000\n",
      "Epoch 43900/50000, Loss: 3797202.5000\n",
      "Epoch 44000/50000, Loss: 3791652.2500\n",
      "Epoch 44100/50000, Loss: 3786501.2500\n",
      "Epoch 44200/50000, Loss: 3780658.0000\n",
      "Epoch 44300/50000, Loss: 3775553.5000\n",
      "Epoch 44400/50000, Loss: 3770328.0000\n",
      "Epoch 44500/50000, Loss: 3766255.7500\n",
      "Epoch 44600/50000, Loss: 3760775.5000\n",
      "Epoch 44700/50000, Loss: 3755531.2500\n",
      "Epoch 44800/50000, Loss: 3750510.5000\n",
      "Epoch 44900/50000, Loss: 3746210.5000\n",
      "Epoch 45000/50000, Loss: 3741120.2500\n",
      "Epoch 45100/50000, Loss: 3736430.7500\n",
      "Epoch 45200/50000, Loss: 3732156.5000\n",
      "Epoch 45300/50000, Loss: 3727124.2500\n",
      "Epoch 45400/50000, Loss: 3722628.5000\n",
      "Epoch 45500/50000, Loss: 3717203.0000\n",
      "Epoch 45600/50000, Loss: 3711809.7500\n",
      "Epoch 45700/50000, Loss: 3706390.0000\n",
      "Epoch 45800/50000, Loss: 3701394.7500\n",
      "Epoch 45900/50000, Loss: 3695922.2500\n",
      "Epoch 46000/50000, Loss: 3689608.2500\n",
      "Epoch 46100/50000, Loss: 3683466.7500\n",
      "Epoch 46200/50000, Loss: 3677140.0000\n",
      "Epoch 46300/50000, Loss: 3671561.5000\n",
      "Epoch 46400/50000, Loss: 3665981.5000\n",
      "Epoch 46500/50000, Loss: 3660532.5000\n",
      "Epoch 46600/50000, Loss: 3654553.0000\n",
      "Epoch 46700/50000, Loss: 3649633.5000\n",
      "Epoch 46800/50000, Loss: 3643910.0000\n",
      "Epoch 46900/50000, Loss: 3639441.0000\n",
      "Epoch 47000/50000, Loss: 3633212.5000\n",
      "Epoch 47100/50000, Loss: 3626571.7500\n",
      "Epoch 47200/50000, Loss: 3620568.5000\n",
      "Epoch 47300/50000, Loss: 3614698.7500\n",
      "Epoch 47400/50000, Loss: 3609293.5000\n",
      "Epoch 47500/50000, Loss: 3601935.7500\n",
      "Epoch 47600/50000, Loss: 3595244.7500\n",
      "Epoch 47700/50000, Loss: 3588862.5000\n",
      "Epoch 47800/50000, Loss: 3584097.0000\n",
      "Epoch 47900/50000, Loss: 3577687.7500\n",
      "Epoch 48000/50000, Loss: 3572015.7500\n",
      "Epoch 48100/50000, Loss: 3566081.7500\n",
      "Epoch 48200/50000, Loss: 3560581.0000\n",
      "Epoch 48300/50000, Loss: 3553029.7500\n",
      "Epoch 48400/50000, Loss: 3546910.2500\n",
      "Epoch 48500/50000, Loss: 3540467.5000\n",
      "Epoch 48600/50000, Loss: 3534955.0000\n",
      "Epoch 48700/50000, Loss: 3529189.2500\n",
      "Epoch 48800/50000, Loss: 3522525.5000\n",
      "Epoch 48900/50000, Loss: 3516219.7500\n",
      "Epoch 49000/50000, Loss: 3510975.7500\n",
      "Epoch 49100/50000, Loss: 3503983.2500\n",
      "Epoch 49200/50000, Loss: 3498541.2500\n",
      "Epoch 49300/50000, Loss: 3493037.0000\n",
      "Epoch 49400/50000, Loss: 3487500.5000\n",
      "Epoch 49500/50000, Loss: 3481981.2500\n",
      "Epoch 49600/50000, Loss: 3475835.7500\n",
      "Epoch 49700/50000, Loss: 3470522.5000\n",
      "Epoch 49800/50000, Loss: 3464143.7500\n",
      "Epoch 49900/50000, Loss: 3458085.2500\n",
      "Epoch 50000/50000, Loss: 3451924.7500\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "epochs = 50000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100==0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6ac00d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "model.eval()\n",
    "y_pred = model(X_test_tensor).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5273cfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 47672744.0000\n",
      "Test RMSE: 6904.5452\n",
      "Test MAE: 4811.9199\n",
      "Test R²: 0.6929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "y_test_numpy=y_test_tensor.numpy()\n",
    "\n",
    "\n",
    "# calculate metrics\n",
    "mse = mean_squared_error(y_test_numpy, y_pred)\n",
    "rmse=mse**0.5\n",
    "mae = mean_absolute_error(y_test_numpy, y_pred)\n",
    "r2 = r2_score(y_test_numpy, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "print(f\"Test R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d657b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_charges(age, sex, bmi, children, smoker, region):\n",
    "    # create a DataFrame for the input\n",
    "    input_df = pd.DataFrame({\n",
    "        'age': [age],\n",
    "        'sex': [sex],\n",
    "        'bmi': [bmi],\n",
    "        'children': [children],\n",
    "        'smoker': [smoker],\n",
    "        'region': [region]\n",
    "    })\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        input_df[feature] = label_encoder[feature].transform(input_df[feature])\n",
    "\n",
    "    input_df = scaler.transform(input_df)\n",
    "    input_tensor = torch.tensor(input_df, dtype=torch.float32)\n",
    "    predicted_charge = model(input_tensor).item()\n",
    "    return predicted_charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "55e3ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted insurance charge: $14531.51\n"
     ]
    }
   ],
   "source": [
    "predicted=predict_charges(19, 'male', 27.9, 0, 'yes', 'southwest')\n",
    "print(f\"Predicted insurance charge: ${predicted:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9053062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
