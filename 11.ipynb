{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38730f9a",
   "metadata": {},
   "source": [
    "#### Breakdown of a simple neural network\n",
    "\n",
    "* X  --> input\n",
    "* Wx --> weights\n",
    "* bx --> bias\n",
    "* A  --> activation function\n",
    "* Y  --> output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0bef84",
   "metadata": {},
   "source": [
    "##### Forward Propogation\n",
    "- Z = W1.X + b1 \n",
    "- Z' = A(Z)\n",
    "- Y= W2.Z' + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e1ffc",
   "metadata": {},
   "source": [
    "- Loss function\n",
    "- Back-propogation\n",
    "- Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5c033",
   "metadata": {},
   "source": [
    "#### Components of pytorch\n",
    "- base class for defining custom models --> torch.nn.module\n",
    "- fully connected (dense layers) --> torch.nn.linear\n",
    "- activation function --> torch.nn.ReLU\n",
    "- optimizers --> torch.optim\n",
    "- loss function --> torch.nn.CrossEntropyLoss\n",
    "- load data in batch ---> torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50f55e",
   "metadata": {},
   "source": [
    "#### Different ways to create a neural network\n",
    "- Function --> flexible, harder to interpret \n",
    "- Sequential --> nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a5a78",
   "metadata": {},
   "source": [
    "## Building a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31734aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72eb267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API --> allows us to define the forward pass using functions instead of layers. This can be useful for more complex architectures or when we want to have more control over the forward pass.\n",
    "\n",
    "class SimpleNNFunct(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNNFunct, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994be016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential API --> more concise, but less flexible\n",
    "\n",
    "class SimpleNNSeq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNNSeq, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae014e3a",
   "metadata": {},
   "source": [
    "#### Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7132dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNNFunct(\n",
      "  (fc1): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_func=SimpleNNFunct(input_size=4, hidden_size=10, output_size=3)\n",
    "print(model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52643084",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.randn(10, 4) # 10 samples, 4 features each\n",
    "Y=torch.randint(0, 3, (10,)) # 10 samples, class labels for 0, 1, and 2 (for 3 classes)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model_func.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310247dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1259, -0.9134,  1.9818, -0.0125],\n",
      "        [-0.6460, -1.4251, -0.4740,  0.3760],\n",
      "        [-1.2047, -0.3674,  0.0334,  0.4371],\n",
      "        [ 0.8840,  0.1075,  0.5259, -0.9913],\n",
      "        [-2.2000,  0.4255,  2.3301, -1.1822],\n",
      "        [-0.4727,  1.4254,  0.6601,  0.3892],\n",
      "        [ 1.1757, -0.5858, -0.6647,  1.0112],\n",
      "        [ 0.9328, -2.2772, -0.1582,  0.5953],\n",
      "        [ 0.8087,  0.5762,  0.3558,  2.3565],\n",
      "        [ 0.0443, -1.5602, -0.1148, -1.3638]])\n",
      "tensor([0, 0, 0, 1, 2, 0, 2, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d905a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/15000], Loss: 0.04086\n",
      "Epoch [200/15000], Loss: 0.03284\n",
      "Epoch [300/15000], Loss: 0.02684\n",
      "Epoch [400/15000], Loss: 0.02227\n",
      "Epoch [500/15000], Loss: 0.01872\n",
      "Epoch [600/15000], Loss: 0.01590\n",
      "Epoch [700/15000], Loss: 0.01363\n",
      "Epoch [800/15000], Loss: 0.01179\n",
      "Epoch [900/15000], Loss: 0.01027\n",
      "Epoch [1000/15000], Loss: 0.00900\n",
      "Epoch [1100/15000], Loss: 0.00793\n",
      "Epoch [1200/15000], Loss: 0.00703\n",
      "Epoch [1300/15000], Loss: 0.00626\n",
      "Epoch [1400/15000], Loss: 0.00559\n",
      "Epoch [1500/15000], Loss: 0.00502\n",
      "Epoch [1600/15000], Loss: 0.00452\n",
      "Epoch [1700/15000], Loss: 0.00408\n",
      "Epoch [1800/15000], Loss: 0.00370\n",
      "Epoch [1900/15000], Loss: 0.00336\n",
      "Epoch [2000/15000], Loss: 0.00306\n",
      "Epoch [2100/15000], Loss: 0.00279\n",
      "Epoch [2200/15000], Loss: 0.00256\n",
      "Epoch [2300/15000], Loss: 0.00234\n",
      "Epoch [2400/15000], Loss: 0.00215\n",
      "Epoch [2500/15000], Loss: 0.00198\n",
      "Epoch [2600/15000], Loss: 0.00182\n",
      "Epoch [2700/15000], Loss: 0.00168\n",
      "Epoch [2800/15000], Loss: 0.00155\n",
      "Epoch [2900/15000], Loss: 0.00143\n",
      "Epoch [3000/15000], Loss: 0.00133\n",
      "Epoch [3100/15000], Loss: 0.00123\n",
      "Epoch [3200/15000], Loss: 0.00114\n",
      "Epoch [3300/15000], Loss: 0.00106\n",
      "Epoch [3400/15000], Loss: 0.00099\n",
      "Epoch [3500/15000], Loss: 0.00092\n",
      "Epoch [3600/15000], Loss: 0.00085\n",
      "Epoch [3700/15000], Loss: 0.00080\n",
      "Epoch [3800/15000], Loss: 0.00074\n",
      "Epoch [3900/15000], Loss: 0.00069\n",
      "Epoch [4000/15000], Loss: 0.00065\n",
      "Epoch [4100/15000], Loss: 0.00060\n",
      "Epoch [4200/15000], Loss: 0.00057\n",
      "Epoch [4300/15000], Loss: 0.00053\n",
      "Epoch [4400/15000], Loss: 0.00049\n",
      "Epoch [4500/15000], Loss: 0.00046\n",
      "Epoch [4600/15000], Loss: 0.00043\n",
      "Epoch [4700/15000], Loss: 0.00041\n",
      "Epoch [4800/15000], Loss: 0.00038\n",
      "Epoch [4900/15000], Loss: 0.00036\n",
      "Epoch [5000/15000], Loss: 0.00034\n",
      "Epoch [5100/15000], Loss: 0.00032\n",
      "Epoch [5200/15000], Loss: 0.00030\n",
      "Epoch [5300/15000], Loss: 0.00028\n",
      "Epoch [5400/15000], Loss: 0.00026\n",
      "Epoch [5500/15000], Loss: 0.00025\n",
      "Epoch [5600/15000], Loss: 0.00023\n",
      "Epoch [5700/15000], Loss: 0.00022\n",
      "Epoch [5800/15000], Loss: 0.00020\n",
      "Epoch [5900/15000], Loss: 0.00019\n",
      "Epoch [6000/15000], Loss: 0.00018\n",
      "Epoch [6100/15000], Loss: 0.00017\n",
      "Epoch [6200/15000], Loss: 0.00016\n",
      "Epoch [6300/15000], Loss: 0.00015\n",
      "Epoch [6400/15000], Loss: 0.00014\n",
      "Epoch [6500/15000], Loss: 0.00013\n",
      "Epoch [6600/15000], Loss: 0.00013\n",
      "Epoch [6700/15000], Loss: 0.00012\n",
      "Epoch [6800/15000], Loss: 0.00011\n",
      "Epoch [6900/15000], Loss: 0.00011\n",
      "Epoch [7000/15000], Loss: 0.00010\n",
      "Epoch [7100/15000], Loss: 0.00009\n",
      "Epoch [7200/15000], Loss: 0.00009\n",
      "Epoch [7300/15000], Loss: 0.00008\n",
      "Epoch [7400/15000], Loss: 0.00008\n",
      "Epoch [7500/15000], Loss: 0.00007\n",
      "Epoch [7600/15000], Loss: 0.00007\n",
      "Epoch [7700/15000], Loss: 0.00007\n",
      "Epoch [7800/15000], Loss: 0.00006\n",
      "Epoch [7900/15000], Loss: 0.00006\n",
      "Epoch [8000/15000], Loss: 0.00006\n",
      "Epoch [8100/15000], Loss: 0.00005\n",
      "Epoch [8200/15000], Loss: 0.00005\n",
      "Epoch [8300/15000], Loss: 0.00005\n",
      "Epoch [8400/15000], Loss: 0.00004\n",
      "Epoch [8500/15000], Loss: 0.00004\n",
      "Epoch [8600/15000], Loss: 0.00004\n",
      "Epoch [8700/15000], Loss: 0.00004\n",
      "Epoch [8800/15000], Loss: 0.00004\n",
      "Epoch [8900/15000], Loss: 0.00003\n",
      "Epoch [9000/15000], Loss: 0.00003\n",
      "Epoch [9100/15000], Loss: 0.00003\n",
      "Epoch [9200/15000], Loss: 0.00003\n",
      "Epoch [9300/15000], Loss: 0.00003\n",
      "Epoch [9400/15000], Loss: 0.00003\n",
      "Epoch [9500/15000], Loss: 0.00002\n",
      "Epoch [9600/15000], Loss: 0.00002\n",
      "Epoch [9700/15000], Loss: 0.00002\n",
      "Epoch [9800/15000], Loss: 0.00002\n",
      "Epoch [9900/15000], Loss: 0.00002\n",
      "Epoch [10000/15000], Loss: 0.00002\n",
      "Epoch [10100/15000], Loss: 0.00002\n",
      "Epoch [10200/15000], Loss: 0.00002\n",
      "Epoch [10300/15000], Loss: 0.00002\n",
      "Epoch [10400/15000], Loss: 0.00001\n",
      "Epoch [10500/15000], Loss: 0.00001\n",
      "Epoch [10600/15000], Loss: 0.00001\n",
      "Epoch [10700/15000], Loss: 0.00001\n",
      "Epoch [10800/15000], Loss: 0.00001\n",
      "Epoch [10900/15000], Loss: 0.00001\n",
      "Epoch [11000/15000], Loss: 0.00001\n",
      "Epoch [11100/15000], Loss: 0.00001\n",
      "Epoch [11200/15000], Loss: 0.00001\n",
      "Epoch [11300/15000], Loss: 0.00001\n",
      "Epoch [11400/15000], Loss: 0.00001\n",
      "Epoch [11500/15000], Loss: 0.00001\n",
      "Epoch [11600/15000], Loss: 0.00001\n",
      "Epoch [11700/15000], Loss: 0.00001\n",
      "Epoch [11800/15000], Loss: 0.00001\n",
      "Epoch [11900/15000], Loss: 0.00001\n",
      "Epoch [12000/15000], Loss: 0.00001\n",
      "Epoch [12100/15000], Loss: 0.00001\n",
      "Epoch [12200/15000], Loss: 0.00001\n",
      "Epoch [12300/15000], Loss: 0.00001\n",
      "Epoch [12400/15000], Loss: 0.00000\n",
      "Epoch [12500/15000], Loss: 0.00000\n",
      "Epoch [12600/15000], Loss: 0.00000\n",
      "Epoch [12700/15000], Loss: 0.00000\n",
      "Epoch [12800/15000], Loss: 0.00000\n",
      "Epoch [12900/15000], Loss: 0.00000\n",
      "Epoch [13000/15000], Loss: 0.00000\n",
      "Epoch [13100/15000], Loss: 0.00000\n",
      "Epoch [13200/15000], Loss: 0.00000\n",
      "Epoch [13300/15000], Loss: 0.00000\n",
      "Epoch [13400/15000], Loss: 0.00000\n",
      "Epoch [13500/15000], Loss: 0.00000\n",
      "Epoch [13600/15000], Loss: 0.00000\n",
      "Epoch [13700/15000], Loss: 0.00000\n",
      "Epoch [13800/15000], Loss: 0.00000\n",
      "Epoch [13900/15000], Loss: 0.00000\n",
      "Epoch [14000/15000], Loss: 0.00000\n",
      "Epoch [14100/15000], Loss: 0.00000\n",
      "Epoch [14200/15000], Loss: 0.00000\n",
      "Epoch [14300/15000], Loss: 0.00000\n",
      "Epoch [14400/15000], Loss: 0.00000\n",
      "Epoch [14500/15000], Loss: 0.00000\n",
      "Epoch [14600/15000], Loss: 0.00000\n",
      "Epoch [14700/15000], Loss: 0.00000\n",
      "Epoch [14800/15000], Loss: 0.00000\n",
      "Epoch [14900/15000], Loss: 0.00000\n",
      "Epoch [15000/15000], Loss: 0.00000\n",
      "Final loss: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epoch=15000\n",
    "for e in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_func(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (e+1) % 100 == 0:\n",
    "        print(f\"Epoch [{e+1}/{epoch}], Loss: {loss.item():.5f}\")\n",
    "print(f\"Final loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b444b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
