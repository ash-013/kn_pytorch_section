{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedf700a",
   "metadata": {},
   "source": [
    "# Manipulating tensors\n",
    "- reshaping\n",
    "- slicing\n",
    "- joining or splitting \n",
    "- transposing and permuting dimension\n",
    "- cloning and detaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3260491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cu126\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec3ddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: \n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# reshaping tensors --> reshape and view\n",
    "original_tensor=torch.arange(12) # 1D tensor with values from 0 to 11\n",
    "print(f\"Original tensor: \\n{original_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d91ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([12])\n",
      "12\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(original_tensor.ndim)\n",
    "print(original_tensor.shape)\n",
    "print(original_tensor.nelement())\n",
    "print(original_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e6d22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped tensor: \n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "2\n",
      "torch.Size([3, 4])\n",
      "12\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# changing the dimention of a tensor\n",
    "reshaped_tensor=original_tensor.reshape(3,4) # reshape to 3 rows and 4 columns\n",
    "print(f\"Reshaped tensor: \\n{reshaped_tensor}\")\n",
    "print(reshaped_tensor.ndim)\n",
    "print(reshaped_tensor.shape)\n",
    "print(reshaped_tensor.nelement())\n",
    "print(reshaped_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d788577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped tensor: \n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "2\n",
      "torch.Size([2, 6])\n",
      "12\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "reshaped_tensor_1=original_tensor.reshape(2,6) # reshape to 2 rows and 6 columns\n",
    "print(f\"Reshaped tensor: \\n{reshaped_tensor_1}\")\n",
    "print(reshaped_tensor_1.ndim)\n",
    "print(reshaped_tensor_1.shape)\n",
    "print(reshaped_tensor_1.nelement())\n",
    "print(reshaped_tensor_1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e121136c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewed tensor: \n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "2\n",
      "torch.Size([3, 4])\n",
      "12\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# view operation --> returns a new tensor with the same data as the original tensor but with a different shape, requires the original tensor to be contiguous in memory\n",
    "\n",
    "viewed_tensor=original_tensor.view(3,4) # view to 3 rows and 4 columns\n",
    "print(f\"Viewed tensor: \\n{viewed_tensor}\")\n",
    "print(viewed_tensor.ndim)\n",
    "print(viewed_tensor.shape)\n",
    "print(viewed_tensor.nelement())\n",
    "print(viewed_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "730c4dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped tensor with -1: \n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "2\n",
      "torch.Size([4, 3])\n",
      "12\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#  -1--> infer the size of the dimension based on the number of elements in the original tensor and the specified dimensions\n",
    "reshaped_tensor_2=original_tensor.reshape(4,-1) # reshape to 4 rows and infer the number of columns\n",
    "print(f\"Reshaped tensor with -1: \\n{reshaped_tensor_2}\")\n",
    "print(reshaped_tensor_2.ndim)\n",
    "print(reshaped_tensor_2.shape)\n",
    "print(reshaped_tensor_2.nelement())\n",
    "print(reshaped_tensor_2.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ddd6941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened tensor: \n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "1\n",
      "torch.Size([12])\n",
      "12\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "flattened_tensor=original_tensor.reshape(-1) # flatten the tensor to 1D\n",
    "print(f\"Flattened tensor: \\n{flattened_tensor}\")\n",
    "print(flattened_tensor.ndim)\n",
    "print(flattened_tensor.shape)\n",
    "print(flattened_tensor.nelement())\n",
    "print(flattened_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d058de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check wether the original tensor is contiguous in memory\n",
    "# reshaping a tensor with reshape does not change the memory layout of the original tensor, but view requires the original tensor to be contiguous in memory, view does not work if the original tensor is not contiguous in memory, but reshape works regardless of the memory layout of the original tensor\n",
    "print(original_tensor.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627580a",
   "metadata": {},
   "source": [
    "#### Slicing operation\n",
    "- we extract specific portions of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c8b5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tensor_a=torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(tensor_a)\n",
    "print(tensor_a.is_contiguous())\n",
    "print(tensor_a.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "852efa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([1, 4, 7])\n",
      "tensor([3, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_a[0])\n",
    "print(tensor_a[:,0])\n",
    "print(tensor_a[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32b9be9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([3, 3])\n",
      "8\n",
      "9\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(tensor_a.ndim)\n",
    "print(tensor_a.shape)\n",
    "print(tensor_a.element_size())\n",
    "print(tensor_a.nelement())\n",
    "print(tensor_a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba31b29",
   "metadata": {},
   "source": [
    "#### Joining tensors\n",
    "- torch.cat() ---> merges the tensors along an existing dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff109456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "tensor_1=torch.tensor([[1,2],[3,4]])\n",
    "tensor_2=torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "print(tensor_1)\n",
    "print(tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5d33343",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_tensor_rows=torch.cat((tensor_1,tensor_2),dim=0) # concatenate along the rows (dim=0 --> vertical concatenation)\n",
    "concat_tensor_cols=torch.cat((tensor_1,tensor_2),dim=1) # concatenate along the columns (dim=1 --> horizontal concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ee1b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged along rows:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "\n",
      "\n",
      "Merged along columns:\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Merged along rows:\\n\"+str(concat_tensor_rows))\n",
    "print(\"\\n\\nMerged along columns:\\n\"+str(concat_tensor_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc05d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack --> creates a new dimension which increases the tensor's rank by 1, stack requires the input tensors to have the same shape, stack concatenates the input tensors along a new dimension, the new dimension is specified by the dim argument, if dim=0 --> stack along the first dimension, if dim=1 --> stack along the second dimension, and so on\n",
    "\n",
    "\n",
    "stack_tensor_rows=torch.stack((tensor_1,tensor_2),dim=0)\n",
    "stack_tensor_cols=torch.stack((tensor_1,tensor_2),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f89477ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_1.shape)\n",
    "print(tensor_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7cf9394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(stack_tensor_rows.shape)\n",
    "print(stack_tensor_cols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28180b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_1)\n",
    "print(tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9079167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "tensor([[[1, 2],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[3, 4],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "print(stack_tensor_rows)\n",
    "print(stack_tensor_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a835d",
   "metadata": {},
   "source": [
    "#### Splitting tensors\n",
    "- torch.chunk()  ---> divides the tensor into equal sized chunks\n",
    "- torch.spilit() ---> allows uneven splitting based on size of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1b87273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([ 9, 10, 11]))\n",
      "tensor([0, 1, 2])\n",
      "tensor([3, 4, 5])\n",
      "tensor([6, 7, 8])\n",
      "tensor([ 9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# torch.chunk()  --> splits a tensor into a specified number of chunks along a specified dimension, the number of chunks is specified by the chunks argument, the dimension along which to split the tensor is specified by the dim argument, if dim=0 --> split along the first dimension, if dim=1 --> split along the second dimension, and so on, the last chunk may be smaller than the others if the tensor cannot be evenly divided by the number of chunks\n",
    "\n",
    "\n",
    "orig_tensor=torch.arange(12)\n",
    "chunks=torch.chunk(orig_tensor,4, dim=0) # split the tensor into 4 chunks along the first dimension (dim=0 by default)\n",
    "print(chunks)\n",
    "\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a32b35e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2, 3, 4, 5, 6, 7]), tensor([8, 9]))\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "tensor([8, 9])\n"
     ]
    }
   ],
   "source": [
    "# torch.split() --> splits a tensor into chunks of a specified size along a specified dimension, the size of each chunk is specified by the split_size argument, the dimension along which to split the tensor is specified by the dim argument, if dim=0 --> split along the first dimension, if dim=1 --> split along the second dimension, and so on, the last chunk may be smaller than the others if the tensor cannot be evenly divided by the split size\n",
    "\n",
    "\n",
    "original_tensor=torch.arange(10)\n",
    "split_tensor=torch.split(original_tensor, 8, dim=0) # split the tensor into chunks of size 8 along the first dimension (dim=0 by default)\n",
    "print(split_tensor)\n",
    "\n",
    "for split in split_tensor:\n",
    "    print(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852e24e",
   "metadata": {},
   "source": [
    "#### Transposing and permuting\n",
    "- transpose():  swaps the two dimensions, (mxn) --> nxm\n",
    "- permute(): rearranges all dimensions in the specified order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4934ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "original_tensor=torch.arange(24).reshape(2,3,4)\n",
    "print(original_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "987a6343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "\n",
      "\n",
      "\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [12, 13, 14, 15]],\n",
      "\n",
      "        [[ 4,  5,  6,  7],\n",
      "         [16, 17, 18, 19]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [20, 21, 22, 23]]])\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# transpose --> permutes the dimensions of a tensor, the dimensions to be permuted are specified by the dim0 and dim1 arguments, if dim0=0 and dim1=1 --> transpose the first and second dimensions, if dim0=1 and dim1=2 --> transpose the second and third dimensions, and so on\n",
    "\n",
    "transposed_tensor=original_tensor.transpose(0,1) # transpose the first and second dimensions\n",
    "\n",
    "\n",
    "print(original_tensor)\n",
    "print(\"\\n\\n\")\n",
    "print(transposed_tensor)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(original_tensor.shape)\n",
    "print(transposed_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff8cde35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "\n",
      "\n",
      "\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [12, 13, 14, 15]],\n",
      "\n",
      "        [[ 4,  5,  6,  7],\n",
      "         [16, 17, 18, 19]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [20, 21, 22, 23]]])\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# permute --> permutes the dimensions of a tensor, the dimensions to be permuted are specified by the dims argument, which is a tuple of the new order of the dimensions, for example, if dims=(1,0,2) --> permute the first dimension to the second position, the second dimension to the first position, and keep the third dimension in the same position\n",
    "\n",
    "\n",
    "# tensor dimensions:    2,3,4\n",
    "# index of dimensions:  0,1,2\n",
    "\n",
    "# at index 0 --> 2\n",
    "# at index 1 --> 3\n",
    "# at index 2 --> 4\n",
    "\n",
    "# now we want to permute the dimensions to be in the order of 1,0,2 --> the first dimension (2) will be moved to the second position, the second dimension (3) will be moved to the first position, and the third dimension (4) will remain in the same position\n",
    "\n",
    "# after permuting the dimensions, the new order of the dimensions will be 3,2,4 --> the first dimension will be 3, the second dimension will be 2, and the third dimension will be 4\n",
    "\n",
    "\n",
    "original_tensor=torch.arange(24).reshape(2,3,4)\n",
    "print(original_tensor)\n",
    "print(\"\\n\\n\")\n",
    "permuted_tensor=original_tensor.permute(1,0,2) # permute the first dimension to the second position, the second dimension to the first position, and keep the third dimension in the same position\n",
    "print(permuted_tensor)\n",
    "print(\"\\n\\n\")\n",
    "print(original_tensor.shape)\n",
    "print(permuted_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64574d",
   "metadata": {},
   "source": [
    "#### cloning and detaching tensors \n",
    "\n",
    "- cloning a tensor creates a copy of the original tensor with the same data and the same computational graph, \n",
    " \n",
    "- detaching a tensor creates a new tensor that shares the same data as the original tensor but does not require gradients and is not part of the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2bec0f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 2, 3]\n",
      "134998921451392\n",
      "134998921452672\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "a=[1,2,3]\n",
    "b=copy.deepcopy(a)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(id(a))\n",
    "print(id(b))\n",
    "\n",
    "print(a == b)\n",
    "print(a is b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f451f2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensor=torch.ones(3,3, requires_grad=True) # part of computation graph, so it will be updated during backpropagation, and it will contribute to the gradients of the original tensor\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2205f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "False\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cloned_tensor=tensor.clone() # clone creates a copy of the tensor with the same data and the same requires_grad property, but it does not share the same memory as the original tensor, so changes to the cloned tensor will not affect the original tensor, and vice versa. Part of computation graph, so it will be updated during backpropagation, and it will contribute to the gradients of the original tensor\n",
    "\n",
    "print(cloned_tensor)\n",
    "print(cloned_tensor.requires_grad)\n",
    "\n",
    "\n",
    "tensor.detach_() # detach creates a new tensor that shares the same data as the original tensor but does not require gradients, changes to the detached tensor will affect the original tensor, but changes to the original tensor will not affect the detached tensor, detached from the computation graph, so it will not be updated during backpropagation, and it will not contribute to the gradients of the original tensor, but storage is shared between the original tensor and the detached tensor, so changes to the data of the detached tensor will affect the original tensor, and changes to the data of the original tensor will affect the detached tensor\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
